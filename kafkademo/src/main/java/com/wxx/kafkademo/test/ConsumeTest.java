package com.wxx.kafkademo.test;

import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.TopicPartitionInfo;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.junit.Before;
import org.junit.Test;
import org.junit.runner.RunWith;
import org.springframework.boot.test.context.SpringBootTest;
import org.springframework.test.context.junit4.SpringRunner;

import java.time.Duration;
import java.util.*;
import java.util.regex.Pattern;

/**
 * @PakageName
 * @author: xiuxu.wang
 * @description
 * @ClassName
 * @date: 2020-06-15 11:21
 **/
@SpringBootTest
@RunWith(SpringRunner.class)
public class ConsumeTest {

    KafkaConsumer kafkaConsumer;
    @Before
    public void connect(){
        Properties properties = new Properties();
        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        //æŒ‡å®šæ¶ˆè´¹è€…ç»„çš„åå­—
        properties.put(ConsumerConfig.GROUP_ID_CONFIG, "consumer_test1");

        //ğŸ“¶æ¶ˆè´¹è€…åˆ†åŒºç­–ç•¥
        properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, "");

        //å…³é—­ä½ç§»çš„è‡ªåŠ¨æäº¤
        properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);

        //è®¾ç½®ä¸ºæ–°çš„æ¶ˆè´¹è€…ç»„çš„è¯»å–æ•°æ®ä¸ºèµ·å§‹ä½ç½®
        //å«earliestè€Œä¸æ˜¯0æ˜¯å› ä¸ºpartitionçš„åˆ é™¤ç­–ç•¥å¯¼è‡´å¼€å§‹è¯»çš„æ•°æ®ä¸æ˜¯ç¬¬ä¸€æ¡
        properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        kafkaConsumer = new KafkaConsumer(properties);

    }
    @Test
    public void consume(){
        //è®¢é˜…ä¸»é¢˜å’Œåˆ†åŒº å‚æ•°ï¼ˆä¸»é¢˜ï¼Œå†å‡è¡¡çš„ç›‘å¬å™¨ï¼‰ å¤šä¸ªè®¢é˜…æ–¹æ³•åŒæ—¶ä½¿ç”¨åªä¼šè®¢é˜…æœ€åä¸€ä¸ª è®¢é˜…å¤šä¸ªç”¨pattençš„é‡è½½æ–¹æ³•
        kafkaConsumer.subscribe(Collections.singletonList("topic-demo"), new ConsumerRebalanceListener() {
            @Override
            public void onPartitionsRevoked(Collection<TopicPartition> collection) {

            }

            @Override
            public void onPartitionsAssigned(Collection<TopicPartition> collection) {

            }
        });
        //kafkaæ¶ˆè´¹ç«¯æ¶ˆè´¹ç¬¦åˆæ­£åˆ™çš„ä¸»é¢˜
        kafkaConsumer.subscribe(Pattern.compile("topic-*"));
        //assignçš„æ–¹å¼è®¢é˜… æŒ‡å®šåˆ†åŒºæ¶ˆè´¹
        //ç±»ä¼¼ä¸€ä¸ªå®ä½“çš„æ¦‚å¿µå¯¹è±¡
        TopicPartition tp = new TopicPartition("topic-demo", 1);
        kafkaConsumer.assign(Collections.singleton(tp));

        //æŒ‡å®šå“ªä¸ªåˆ†åŒºoffsetæ¶ˆè´¹
        kafkaConsumer.seek(new TopicPartition("topic-demo",2),0);

        //è¿”å›æŒ‡å®šä¸»é¢˜çš„åˆ†åŒºå…ƒæ•°æ®ä¿¡æ¯
        List<TopicPartitionInfo> demo = kafkaConsumer.partitionsFor("demo");

        //kafkaå–æ¶ˆè®¢é˜…
        kafkaConsumer.unsubscribe();
        //assignmentè·å–çš„æ˜¯kafkaåˆ†é…çš„åˆ†åŒºä¿¡æ¯
        Set<TopicPartition> assignment = kafkaConsumer.assignment();

        try {
            while (true) {
                //æ¶ˆæ¯çš„æ¶ˆè´¹å’ŒProducerRecordå¯¹åº” æ¶ˆè´¹çš„é˜»å¡æ—¶é—´
                //æ‹¿åˆ°çš„æ˜¯æ¶ˆæ¯å¯¹è±¡çš„é›†åˆ åŒ…å«topicçš„åˆ†åŒºï¼Œkeyï¼Œvalueç­‰å…ƒä¿¡æ¯
                ConsumerRecords<String, String> records = kafkaConsumer.poll(Duration.ofMillis(100));
                for (ConsumerRecord<String, String> record : records) {
                    String topic = record.topic();
                    int partition = record.partition();
                    long offset = record.offset();
                }

                //è·å–æ¶ˆè´¹é›†çš„æ‰€æœ‰åˆ†åŒº
                Set<TopicPartition> partitions = records.partitions();
                //è·å–æ¶ˆè´¹é›†çš„æŒ‡å®šåˆ†åŒºæ¶ˆæ¯
                for (TopicPartition partition : partitions) {
                    //æ¶ˆè´¹é›†
                    List<ConsumerRecord<String, String>> consumerRecords = records.records(partition);
                    for (ConsumerRecord<String, String> consumerRecord : consumerRecords) {
                        System.out.println("åˆ†åŒºï¼š"+consumerRecord.partition()+" ä¸»é¢˜ï¼š"+consumerRecord.topic());
                        //å¯ä»¥æŒ‰ç…§åˆ†åŒºç²’åº¦è¿›è¡ŒåŒæ­¥æäº¤
                    }
                    long lastOffset = consumerRecords.get(consumerRecords.size() - 1).offset();
                    //åŒæ­¥æäº¤äº†æ¯ä¸ªåˆ†åŒºä¸‹çš„ä½ç§»
                    kafkaConsumer.commitSync(Collections.singletonMap(partition,new OffsetAndMetadata(lastOffset)));
                }

                //ä½ç§»çš„æäº¤
                //ä½ç§»çš„å»¶è¿Ÿæäº¤ æäº¤çš„offsetæ˜¯å·²ç»æ¶ˆè´¹çš„æ¡æ•°+1
                kafkaConsumer.commitSync(Duration.ofMillis(1000));
                //kafkaConsumer.position(TopicPartition topicPartition) kafkaä¸‹ä¸€æ¬¡è¦æ¶ˆè´¹çš„offset
                //æ¶ˆè´¹é›†ä¸­æ¯ä¸ªåˆ†åŒºå¯¹åº”çš„å·²æäº¤ä½ç§»
                Map committed = kafkaConsumer.committed(partitions);
                //å¯ä»¥æŒ‰ç…§åˆ†åŒºç²’åº¦è¿›è¡ŒåŒæ­¥æäº¤
                kafkaConsumer.commitSync();

                //kafkaæ¶ˆè´¹æš‚åœ
                kafkaConsumer.paused();
                //kafkaæ¢å¤æ¶ˆè´¹
                kafkaConsumer.resume(Collections.singleton(new TopicPartition("demo",2)));

                //kafkaçš„å¼‚æ­¥æäº¤
                kafkaConsumer.commitAsync((map, e) -> {
                    if(null != e) {
                        System.out.println("å…¶å®å¯ä»¥ä¸ç”¨å¤„ç†å› ä¸ºåé¢çš„æäº¤ä¹Ÿä¼šæˆåŠŸçš„");
                    }else {
                        map.forEach((TopicPartition k, OffsetAndMetadata v) ->{
                            System.out.println(k+"-->"+v);
                        });
                    }

                });


            }
        } finally {
            //ä¿è¯offsetä¸€å®šæäº¤
            kafkaConsumer.commitSync();
            kafkaConsumer.close(); //4
        }

    }

}
